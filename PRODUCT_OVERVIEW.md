# LLM Trace Lens

**LLM出力可観測性プラットフォーム**

---

## 製品概要

LLM Trace Lensは、大規模言語モデル（LLM）をプロダクトに組み込む企業向けに設計された**プロキシ型可観測性プラットフォーム**です。

LLMの「嘘」「自信過剰」「機密情報漏洩」といったリスクを検出・防止し、安全で信頼性の高いAIアプリケーションの構築を支援します。

**バージョン**: v0.5.0

---

## 設計思想

LLM Trace Lensは、3つの設計原則に基づいて開発されています。

### 1. 今すぐ動く（即座に価値提供）

複雑な設定や長期的な統合を必要とせず、導入直後から価値を提供します。OpenAI互換APIとして動作するため、既存コードのエンドポイントを1行変更するだけで利用を開始できます。

### 2. 将来も拡張できる（アーキテクチャの柔軟性）

将来の業界標準（内部トレース機能やニューロン可視化など）に対応できるよう、拡張可能なインターフェースを設計時から用意しています。今日のMVPを捨てずに、明日の最先端技術に進化できます。

### 3. 顧客データが資産になる（フィードバックループ）

すべてのトレースを保存・分析可能にすることで、検証精度の継続的な改善を実現します。誤検知フィードバックの収集と分析機能により、運用しながらシステムを最適化できます。

---

## 主要機能

### LLMプロバイダ対応

| プロバイダ | 対応モデル例 | ストリーミング |
|-----------|-------------|--------------|
| OpenAI | GPT-4, GPT-4o, GPT-4o-mini, o1 | 対応 |
| Anthropic | Claude Opus 4, Claude Sonnet 4 | 対応 |
| Google Gemini | Gemini 1.5 Pro, Gemini 2.0 Flash | 対応 |
| DeepSeek | DeepSeek Chat, DeepSeek Reasoner | 対応 |

- **ベンダーフリー設計**: すべてのプロバイダを同一のOpenAI互換インターフェースで利用可能
- **リアルタイムストリーミング**: SSE（Server-Sent Events）による低遅延レスポンス

### 構造化レスポンス強制

LLMの出力を以下の形式に構造化し、機械的な検証を可能にします。

| フィールド | 説明 |
|-----------|------|
| answer | 実際の回答内容 |
| confidence | 回答の信頼度（0-100） |
| evidence | 回答の根拠リスト |
| alternatives | 代替案リスト |

### 検証エンジン

複数の検証モジュールを組み合わせた多層防御を提供します。

| 検証機能 | 説明 |
|---------|------|
| **ConfidenceValidator** | 信頼度とエビデンス数の整合性をチェック。高信頼度にもかかわらずエビデンスが不足している矛盾を検出 |
| **RiskScanner** | PII（個人識別情報）やAPIキーなどの機密情報を検出。英語・日本語両対応 |
| **日本語PII検出** | マイナンバー、銀行口座、電話番号、法人番号などの日本固有の機密情報を検出 |
| **カスタムルール** | ワークスペース毎に正規表現パターンを定義し、業務固有の機密情報をブロック |
| **RiskScorer** | 重み付きスコアリングによるリスクレベル判定。閾値はAdmin APIで管理者のみ設定可能 |
| **LLM-as-Judge評価** | 別のLLMを使用してFaithfulness（忠実性）とAnswer Relevance（回答関連性）を自動評価 |

検証結果は **PASS（通過）/ WARN（警告）/ BLOCK（ブロック）** の3段階で評価されます。

### マルチテナント対応

エンタープライズ利用を想定したワークスペース分離機能を提供します。

- **ワークスペース単位のデータ分離**: トレース、コスト、設定を完全に分離
- **APIキー認証**: ワークスペース毎に発行されるAPIキーで認証
- **ワークスペース別カスタムルール**: 業務要件に応じた検証ルールを個別設定

### リアルタイム通知

リスク検出時の即時通知で、迅速な対応を支援します。

| 連携先 | 機能 |
|-------|------|
| **Slack** | Block Kit形式のリッチな通知。リスクレベル、スコア、詳細を視覚的に表示 |
| **Microsoft Teams** | Adaptive Cards形式のインタラクティブなカード通知 |
| **汎用Webhook** | 任意のエンドポイントへのイベント通知（BLOCK/WARN/COSTイベント対応） |

### コストトラッキングと予算管理

LLM利用コストの可視化と予算超過防止機能を提供します。

- **モデル別コスト集計**: プロバイダ・モデル毎の使用量とコストを自動計算
- **月次予算設定**: 予算上限と閾値アラート（80%、95%など）を設定可能
- **リアルタイム進捗表示**: ダッシュボードで予算消化率を可視化

### 月次PDFレポート

運用状況を把握するための自動レポート生成機能を提供します。

- **レポート内容**: 総リクエスト数、コスト内訳、検証サマリー、平均レイテンシ、予算消化率
- **自動メール送信**: SMTPサーバーを設定することで、月次レポートを自動配信
- **Cronジョブ対応**: スケジューラーによる定期実行

### エンタープライズ認証

企業の認証基盤と連携するSSO機能を提供します。

| 認証方式 | 説明 |
|---------|------|
| **Google OAuth** | Google Workspaceアカウントでログイン |
| **Microsoft Entra ID** | Microsoft 365/Azure ADアカウントでログイン |
| **APIキー認証** | プログラマティックアクセス用の認証方式 |

### ストレージ管理

運用要件に応じた柔軟なストレージ選択を提供します。

| ストレージ | 推奨用途 | 特徴 |
|-----------|---------|------|
| **PostgreSQL**（推奨） | 本番環境 | スケーラビリティ、複雑なクエリ、無制限のデータ保持 |
| **Vercel KV** | 開発・小規模プロジェクト | サーバーレス、ゼロ構成、保持ポリシー付き自動削除 |
| **SQLite** | ローカル開発 | ゼロ構成、単一ファイル |

- **KV保持ポリシー**: 最大トレース数（デフォルト5000件）と保存期間（デフォルト30日）を設定可能
- **使用量監視**: ダッシュボードでストレージ使用率を可視化、閾値超過時に警告表示

### フィードバック収集・分析

検証精度を継続的に改善するためのフィードバックループを提供します。

- **フィードバック種別**: 誤検知（false positive）、検出漏れ（false negative）、正しい判定（correct）
- **パターン分析**: 誤検知パターンを分析し、検証ルールの改善に活用
- **Analytics画面**: ダッシュボードでフィードバック統計を可視化

---

## ターゲットユーザーとユースケース

### 対象ユーザー

- **スタートアップ**: LLMを活用した新規プロダクトを迅速に市場投入したい企業
- **成長企業**: LLM利用のスケールに伴うコストとリスクの管理が必要な企業
- **大企業**: コンプライアンス要件を満たしつつLLMを業務に活用したい企業

### 主要ユースケース

#### 機密情報漏洩防止（金融・医療・法務）

規制の厳しい業界では、LLMが意図せず機密情報を出力するリスクがあります。LLM Trace Lensは、マイナンバー、口座番号、医療情報などを自動検出し、漏洩前にブロックします。

#### コスト最適化

LLMの利用コストは予想以上に膨らむことがあります。プロバイダ・モデル別のコスト可視化と予算アラートにより、コストを予測可能な範囲に維持できます。

#### 監査対応

金融機関や上場企業では、AI利用の監査対応が求められます。すべてのトレースを保存し、月次レポートを自動生成することで、監査要件への対応を効率化します。

#### LLMの信頼性向上

LLMの「嘘」や「自信過剰」は、ユーザー体験を損ない、ビジネスリスクを生みます。信頼度とエビデンスの整合性チェックにより、低品質な回答を事前に検出できます。

---

## 導入の簡単さ

LLM Trace Lensは、既存のLLMアプリケーションへの導入を最小限の変更で実現します。

### ステップ1: エンドポイントの変更

```diff
- const endpoint = 'https://api.openai.com/v1/chat/completions';
+ const endpoint = 'https://your-instance.vercel.app/v1/chat/completions';
```

OpenAI互換APIとして動作するため、SDK変更は不要です。

### ステップ2: Vercelへのデプロイ

ワンクリックでVercelにデプロイ可能。セットアップウィザードで初期設定を完了できます。

### ステップ3: ダッシュボードで確認

Webダッシュボードでトレース、検証結果、コストをリアルタイムに確認できます。

---

## 技術スタック

| レイヤー | 技術 |
|---------|------|
| **バックエンド** | Fastify + TypeScript |
| **フロントエンド** | React + Tailwind CSS |
| **デプロイ** | Vercel Serverless Functions |
| **ストレージ** | PostgreSQL / Vercel KV / SQLite（選択可能） |
| **テスト** | Vitest（138テストケース） |

---

## 将来ビジョン

LLM Trace Lensは、LLM可観測性の進化に合わせて継続的に拡張していきます。

### 短期ロードマップ

- ダッシュボードのリアルタイム更新（WebSocket対応）
- チーム招待機能
- ロールベースアクセス制御（Admin/Member/Viewer）

### 中長期ビジョン

- **L3/L4レベル内部トレース対応**: OpenAIのlogprobs/reasoning_tokens、Anthropic Extended Thinking、Google Grounding Metadataなど、プロバイダが提供する内部トレース情報への対応
- **アクティベーション可視化**: ニューロン活性化パターンの可視化（将来的な標準化を見据えて）

現在の`internalTrace: null`フィールドは、これらの将来機能への移行パスとして設計されています。

---

## ライセンス

MIT License

---

## お問い合わせ

- **ドキュメント**: 本リポジトリの`README.md`および`DESIGN_PHILOSOPHY.md`をご参照ください
- **Issue報告**: GitHub Issuesをご利用ください
- **設計思想の詳細**: `DESIGN_PHILOSOPHY.md`をご覧ください

---

## よくある質問（FAQ）

### 1. 機能・検証精度について

#### Q: 検証エンジンの精度はどの程度か？誤検知（過剰ブロック）や検出漏れの発生率は？自社データでのテストは可能か？

**A:** 検証エンジンは、複数の検証モジュール（ConfidenceValidator、RiskScanner、RiskScorer）を組み合わせた多層防御アーキテクチャを採用しています。誤検知率・検出漏れ率は、運用データに基づいて継続的に改善される設計です。

具体的な改善メカニズムとして、フィードバック機能を提供しています：
- **誤検知報告**: `false_positive`としてマーク
- **検出漏れ報告**: `false_negative`としてマーク
- **正しい判定**: `correct`としてマーク

フィードバックデータは90日間保持され、パターン分析に活用されます。自社データでのテストは、ステージング環境での検証を推奨します。

#### Q: ConfidenceValidatorの「信頼度とエビデンス数の矛盾」は具体的にどのようなケースで検知されるのか？チューニングは可能か？

**A:** 以下のケースで検知されます：

| 条件 | 判定 | 説明 |
|------|------|------|
| `confidence >= 90` かつ `evidence.length < 2` | WARN | 高信頼度なのに根拠が不足 |
| `confidence < 50` | WARN | 低信頼度の回答 |

チューニングは、ワークスペース単位で以下のパラメータを調整可能です：

```json
{
  "scoring_weights": {
    "confidenceWeight": 0.4,
    "evidenceWeight": 0.3,
    "piiWeight": 0.2,
    "historicalWeight": 0.1
  },
  "risk_levels": {
    "highRiskMin": 70,
    "mediumRiskMin": 40,
    "lowRiskMax": 39
  }
}
```

#### Q: 構造化レスポンス強制の影響は？モデルによってはJSON出力の精度が落ちるのではないか？フォールバック機構はあるか？

**A:** 段階的なフォールバック機構を実装しています：

1. **レベル1**: 基本的なJSON形式要求
2. **レベル2**: より厳しい指示（マークダウン禁止）
3. **レベル3**: 最後の手段（最も強い指示）

JSONパースに失敗した場合でも、以下のフォールバック処理が行われます：
```json
{
  "answer": "[元のレスポンス内容]",
  "confidence": 50,
  "evidence": ["Raw response - could not parse structured format"],
  "alternatives": []
}
```

このため、サービスが停止することはありません。

#### Q: 構造化が必須だと、自由形式の応答が必要なユースケースには使えないのでは？

**A:** 構造化レスポンスは、検証とトレース機能の基盤となっています。自由形式の応答は`answer`フィールド内で表現されるため、実質的な制約は最小限です。ただし、完全に非構造化な出力が必要な場合は、プロキシを介さない直接接続をご検討ください。

#### Q: OpenAIのfunction callingやAnthropicのtool useなど、各社固有の高度な機能は使えるのか？

**A:** 現在、以下のプロバイダ固有機能に対応しています：

| プロバイダ | Function Calling | Tool Use | JSON Mode |
|-----------|-----------------|----------|-----------|
| **OpenAI** | ✅ 対応 | ✅ 対応 | ✅ `response_format: { type: 'json_object' }` |
| **Anthropic** | ✅ 対応 | ✅ 対応 | ✅ 自動JSON抽出 |
| **Gemini** | ✅ 対応 | ✅ 対応 | ✅ 手動パース |
| **DeepSeek** | ✅ 対応 | ✅ 対応 | ✅ `response_format: { type: 'json_object' }` |

#### Q: ストリーミング時の構造化レスポンスはどのように処理されるのか？

**A:** ストリーミング処理は以下のフローで行われます：

1. SSE（Server-Sent Events）形式で逐次出力
2. チャンク毎にdelta形式で送信
3. 全チャンク結合後にJSON解析
4. 検証・トレースの実行
5. 最終SSEメッセージに`_trace`フィールドを付加

すべてのプロバイダ（OpenAI、Anthropic、Gemini、DeepSeek）でストリーミング対応済みです。

---

### 2. セキュリティ・コンプライアンス

#### Q: トレースデータ（入力・出力）は保存期間後、完全に削除される保証はあるか？

**A:** ストレージバックエンドにより異なります：

| ストレージ | 保持期間 | 削除方式 |
|-----------|---------|---------|
| **Vercel KV** | 7日（トレース）、90日（フィードバック） | TTL設定による自動削除 |
| **PostgreSQL** | 無制限（デフォルト） | 手動管理または定期バッチ処理 |
| **SQLite** | 無制限 | 手動管理 |

Vercel KVでは、TTL（Time To Live）により自動削除されます。PostgreSQL/SQLiteでは、定期削除スクリプトの実装が必要です。

#### Q: 保存データは暗号化されるのか？(at-rest および in-transit)

**A:**
- **In-transit**: HTTPS/TLS による暗号化（デプロイ環境依存）
- **At-rest**:
  - Vercel KV: Vercelのインフラストラクチャによる暗号化
  - PostgreSQL: データベースの設定に依存（推奨: SSL接続 + ディスク暗号化）
  - SQLite: ファイルシステムレベルの暗号化を推奨

本番環境では、SSL/TLS接続とディスク暗号化の設定を推奨します。

#### Q: 日本の個人情報保護法やGDPRに準拠した設計か？

**A:** LLM Trace Lensは、以下のプライバシー保護機能を提供しています：

- **PII自動検出**: マイナンバー、銀行口座、電話番号、メールアドレス等を検出・ブロック
- **ワークスペース分離**: テナント間のデータ分離
- **データ保持ポリシー**: TTLベースの自動削除（Vercel KV）

ただし、法規制への準拠は、お客様のデータ処理方針とデプロイ構成に依存します。コンプライアンス要件については、法務部門との確認を推奨します。

#### Q: 機密情報（PII）の検出パターンは定期的に更新されるのか？自社固有のパターンを追加できるか？

**A:**

**組み込みPIIパターン**（更新はソフトウェアアップデートに含まれます）：

| 分類 | パターン例 | 動作 |
|------|----------|------|
| **BLOCK** | SSN、クレジットカード、APIキー | ブロック |
| **BLOCK** | マイナンバー（文脈付き）、銀行口座番号 | ブロック |
| **WARN** | メールアドレス、電話番号、郵便番号 | 警告 |

**カスタムパターン追加**: ワークスペース単位で正規表現パターンを追加可能です：

```bash
POST /custom-rules
Body: { "pattern": "SECRET_[A-Z0-9]{32}" }
```

カスタムパターンにマッチした場合は常にBLOCK判定となります。

#### Q: マルチテナント環境での他ワークスペースへのデータ漏洩リスクはどのように防止しているか？

**A:** 以下の分離機構により防止しています：

1. **キー構造によるスコープ化**: `workspace:<workspace_id>:<resource_type>:<resource_id>`
2. **APIキー認証**: 各リクエストはAPIキーからワークスペースIDを解決
3. **独立したデータ領域**: トレース、設定、カスタムパターン、コスト追跡がすべて分離

すべてのAPIコールは認証済みワークスペースのスコープ内でのみ動作します。

#### Q: 監査ログは取得できるか？（誰がいつどの設定を変更したか）

**A:** 現在、完全な監査ログ機能は実装されていません。トレースデータ（時刻、プロバイダ、モデル、検証結果）が疑似的な監査証跡として機能します。

設定変更の監査ログ機能は、今後のロードマップで検討予定です。

---

### 3. 導入・統合の容易さ

#### Q: プロキシとして動作する場合、弊社のネットワーク構成（VPC, プロキシ, ファイアウォール）に影響はないか？

**A:** LLM Trace Lensは、アウトバウンド通信のみを使用するプロキシとして動作します：

- **インバウンド**: お客様のアプリケーションからのHTTPSリクエストを受信
- **アウトバウンド**: LLMプロバイダ（OpenAI、Anthropic等）へのHTTPSリクエストを転送

ファイアウォールでは、以下の宛先への443ポート（HTTPS）アウトバウンド通信を許可してください：
- `api.openai.com`
- `api.anthropic.com`
- `generativelanguage.googleapis.com`
- `api.deepseek.com`

#### Q: オンプレミスや自社クラウド環境へのデプロイは可能か？（Vercel以外のホスティング）

**A:** はい、可能です。LLM Trace LensはNode.js/Fastifyベースのアプリケーションであり、以下の環境にデプロイ可能です：

- **Docker**: コンテナ化してECS、GKE、AKS等で運用
- **VM**: EC2、GCE、Azure VM等での直接デプロイ
- **サーバーレス**: AWS Lambda、Google Cloud Functions（要アダプター）

ストレージはPostgreSQLまたはSQLiteを選択することで、完全なセルフホスティングが可能です。

#### Q: OpenAI SDK以外（例: LangChain, LlamaIndex）からの利用もシームレスにできるか？

**A:** はい、OpenAI互換APIとして動作するため、以下のライブラリから利用可能です：

```python
# LangChain
from langchain.llms import OpenAI
llm = OpenAI(
    api_key="your-key",
    base_url="https://your-instance.vercel.app",
    model="gpt-4"
)

# LlamaIndex（同様のbaseURL設定）
```

エンドポイントを変更するだけで、既存のコードをそのまま利用できます。

#### Q: エラーレスポンスの形式はOpenAIと完全互換か？アプリケーション側のエラーハンドリングに影響しないか？

**A:** エラーレスポンスはJSON形式で返却されます：

```json
{
  "error": "エラーメッセージ"
}
```

HTTPステータスコード：
- **400**: Bad Request（バリデーション失敗）
- **404**: Not Found
- **500**: Internal Server Error

OpenAIの`error.message`形式とは若干異なりますが、標準的なJSONエラー形式のため、多くのHTTPクライアントで問題なく処理できます。

---

### 4. 運用・管理

#### Q: Trace Lens自体がダウンした場合、LLMリクエストは素通りするのか、それともブロックされるのか？（障害耐性）

**A:** プロキシがダウンした場合、**リクエストはブロックされます**（HTTP 5xxエラーを返却）。素通り（パススルー）機能は現在実装されていません。

高可用性が必要な場合は、以下を推奨します：
1. 複数インスタンスでの冗長構成
2. ロードバランサーによるヘルスチェック
3. クライアント側でのフォールバック実装（直接接続への切り替え）

なお、トレース保存処理は非同期で行われるため、ストレージ障害時もレスポンスには影響しません。

#### Q: SLAは提供されるのか？

**A:** オープンソース版ではSLAは提供しておりません。ホスト型サービスのエンタープライズプランにおいて、可用性99.5%以上のSLAを提供する予定です。詳細は個別にお問い合わせください。

#### Q: Slack通知の条件（BLOCKのみ、WARN以上など）をワークスペース単位で細かく設定できるか？

**A:** はい、Webhook設定でイベントタイプを指定できます：

```bash
POST /api/webhook/config
Body: {
  "url": "https://hooks.slack.com/...",
  "events": ["BLOCK", "WARN", "COST_ALERT"]
}
```

指定可能なイベント：
- **BLOCK**: ブロック判定時のみ
- **WARN**: 警告判定時
- **COST_ALERT**: コスト閾値（80%、95%）超過時

イベント配列をカスタマイズすることで、通知条件を制御できます。

#### Q: Webhookのペイロード例は公開されているか？自社の監視システムとの連携は容易か？

**A:** 以下のペイロード形式に対応しています：

**汎用JSON形式**:
```json
{
  "event": "BLOCK",
  "timestamp": "2024-01-01T00:00:00Z",
  "traceId": "req_123...",
  "provider": "openai",
  "model": "gpt-4",
  "risk": "Detected potential credit card",
  "details": { ... }
}
```

**Slack（Block Kit形式）**:
```json
{
  "text": "🚫 LLM Trace Alert: BLOCK",
  "attachments": [{
    "color": "danger",
    "fields": [
      { "title": "Event", "value": "BLOCK", "short": true },
      { "title": "Provider", "value": "openai/gpt-4", "short": true }
    ]
  }]
}
```

**Microsoft Teams（Adaptive Cards）**:
MessageCard形式でリッチな通知を送信します。

#### Q: モデルごとのコスト計算は、プロバイダの価格変更に追随して更新されるのか？

**A:** 料金表はソースコード内で管理されています（`src/cost/pricing.ts`）。プロバイダの価格変更時は、ソフトウェアアップデートで対応します。

現在、ランタイムでの動的更新機能は実装されていません。カスタム料金設定が必要な場合は、フォーク版での運用をご検討ください。

#### Q: 予算アラートはメール以外にも通知可能か？（Slack, Teamsなど）

**A:** はい、予算アラートは`COST_ALERT`イベントとしてWebhookに送信されます。Slack、Microsoft Teams、汎用Webhookのいずれでも受信可能です。

---

### 5. 価格・ライセンス

#### Q: オープンソース版（MIT）とホスト型サービスの違いは？商用利用時のコスト構造は？

**A:**　LLM Trace LensはMITライセンスのオープンソースソフトウェアとして提供されており、無料で自己ホストして商用利用いただけます。一方、クラウドホスト型サービス（SaaS）も提供予定で、運用の手間を省き、すぐに使い始めたいお客様向けです。ホスト型の料金はシンプルな従量課金制または月額プランを予定しています（詳細は下記参照）。

#### Q: リクエスト数、保存容量、ユーザー数など、どの単位で課金される想定か？

**A:**　シンプルな従量課金モデルを採用する予定です。具体的には：

無料枠：月間最初の1,000リクエストまで無料（クレジットカード不要でお試しいただけます）
従量課金：1,000リクエストあたり$0.50（約75円） ※ボリュームディスカウントあり
エンタープライズプラン：SSO、監査ログ、SLA保証、専任サポートを含むカスタムプラン（要相談）
保存容量はリクエスト数に含めて考え、長期保存が必要な場合は別途オプションを検討中です。ユーザー数（シート数）による課金は行わず、利用量に応じたフェアな料金体系を目指しています。

#### Q: 導入支援やトラブルシューティングのサポートは有償か？

**A:**　
コミュニティサポート：GitHub Issuesや Discussions は無料でご利用いただけます。
標準サポート：ホスト型サービスの有料プランにはメールサポート（平日24時間以内対応）が含まれます。
プレミアムサポート：エンタープライズプランでは、専任のサポートエンジニアによる導入支援、トレーニング、SLA付きの緊急対応を提供します（別途契約）。

#### Q: ドキュメントはどこまで整備されているか？（特に日本語対応）

**A:** 現在、以下のドキュメントを提供しています：

- **README.md**: クイックスタートガイド
- **DESIGN_PHILOSOPHY.md**: 設計思想の詳細
- **PRODUCT_OVERVIEW.md**: 製品概要（本ドキュメント）

日本語ドキュメントを中心に整備しています。

---

### 6. 将来性・ロードマップ

#### Q: ロードマップにあるL3/L4内部トレースはいつ頃対応予定か？

**A:** 現在の`internalTrace: null`フィールドは、将来の内部トレース機能への移行パスとして設計されています。対応予定の機能：

- OpenAI: logprobs / reasoning_tokens
- Anthropic: Extended Thinking
- Google: Grounding Metadata

具体的なリリース時期は、各プロバイダのAPI安定性と標準化状況を見ながら決定します。

#### Q: ユーザーからの要望機能（例：カスタムダッシュボード、レポートエクスポート）の優先度は？

**A:** 機能要望はGitHub Issuesで受け付けており、ユーザーの声を基にロードマップを策定しています。優先度は以下の要素で決定します：

多くのユーザーから要望がある機能
製品のビジョンとの整合性
実装の複雑さとインパクト
現在、カスタムダッシュボードやレポートエクスポートは高優先度で検討中です。

#### Q: ユーザーコミュニティはあるか？バグ報告や機能リクエストの受け入れ体制は？

**A:** GitHub Issuesでバグ報告・機能リクエストを受け付けています。プルリクエストも歓迎します。

#### Q: プラグインや拡張機能を開発できる仕組みはあるか？

**A:** 現在、公式のプラグインシステムは提供していません。ただし、以下の拡張ポイントがあります：

- **カスタムパターン**: ワークスペース単位で正規表現パターンを追加
- **Webhook**: 任意のエンドポイントへのイベント転送
- **フォーク**: MITライセンスによるカスタマイズ

将来的にプラグインアーキテクチャの導入を検討しています。

---

**LLM Trace Lens** - 信頼できるAIアプリケーションの構築を支援します。
